{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL,torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Documents/dataset_skin40/Skin40\n"
     ]
    }
   ],
   "source": [
    "datdir = os.path.join(os.getcwd(), 'Skin40')\n",
    "print(datdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinDataset(Dataset):\n",
    "    folds=5\n",
    "    def __init__(self, root , num_classes,fold=0,training=False,transform=None):\n",
    "        self.data_path = []\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((784,784)),\n",
    "                transforms.RandomRotation(5),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        for label in range(num_classes):\n",
    "            self.data_dir = os.path.join(root,os.listdir(root)[label])\n",
    "            self.filename = os.listdir(self.data_dir)\n",
    "            l = len(self.filename)\n",
    "            inter = l//SkinDataset.folds\n",
    "            picked = list(range(inter* fold,inter * (fold+1))) if not training else list(range(0,inter*fold))+list(range(inter*(fold+1),l))\n",
    "\n",
    "            for i in picked:\n",
    "                file_path = os.path.join(self.data_dir , self.filename[i])\n",
    "#                 img = Image.open(fil)\n",
    "                self.data_path.append((file_path, label))\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        ddir , label = self.data_path[index]\n",
    "        img = Image.open(ddir)\n",
    "        result = (self.transform(img) , label)\n",
    "        del img\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n",
      "tensor(184946.6250)\n"
     ]
    }
   ],
   "source": [
    "ds = SkinDataset(os.path.join(os.getcwd(),'Skin40'), 40,training=True)\n",
    "print(len(ds))\n",
    "print(ds[100][0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del training_dataloader\n",
    "training_dataset = SkinDataset(os.path.join(os.getcwd(),'Skin40'), 40, training=True)\n",
    "test_dataset = SkinDataset(os.path.join(os.getcwd(),'Skin40'), 40)\n",
    "training_dataloader = DataLoader(training_dataset , batch_size = 3 , num_workers = 1, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset , batch_size = 3 , num_workers = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_func, optimizer, device):\n",
    "    \"\"\"\n",
    "    train model using loss_fn and optimizer in an epoch.\n",
    "    model: CNN networks\n",
    "    train_loader: a Dataloader object with training data\n",
    "    loss_func: loss function\n",
    "    device: train on cpu or gpu device\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    # train the model using minibatch\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, targets)\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # every 100 iteration, print loss\n",
    "        if (i + 1) % 30 == 0:\n",
    "            print (\"Step [{}/{}] Train Loss: {:.4f}\"\n",
    "                   .format(i+1, len(train_loader), loss.item()))\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    model: CNN networks\n",
    "    val_loader: a Dataloader object with validation data\n",
    "    device: evaluate on cpu or gpu device\n",
    "    return classification accuracy of the model on val dataset\n",
    "    \"\"\"\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "    # context-manager that disabled gradient computation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (images, targets) in enumerate(val_loader):\n",
    "            # device: cpu or gpu\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # return the maximum value of each row of the input tensor in the \n",
    "            # given dimension dim, the second return vale is the index location\n",
    "            # of each maxium value found(argmax)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            \n",
    "            \n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            \n",
    "        accuracy = correct / total\n",
    "        print('Accuracy on Test Set: {:.4f} %'.format(100 * accuracy))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_curve(ys, title):\n",
    "    \"\"\"\n",
    "    plot curlve for Loss and Accuacy\n",
    "    Args:\n",
    "        ys: loss or acc list\n",
    "        title: loss or accuracy\n",
    "    \"\"\"\n",
    "    x = np.array(range(len(ys)))\n",
    "    y = np.array(ys)\n",
    "    plt.plot(x, y, c='b')\n",
    "    plt.axis()\n",
    "    plt.title('{} curve'.format(title))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('{}'.format(title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, num_epochs, optimizer, device):\n",
    "    \"\"\"\n",
    "     train and evaluate an classifier num_epochs times.\n",
    "    n and evaluate an classifier num_epochs times.\n",
    "    We use optimizer and cross entropy loss to train the model. \n",
    "    Args: \n",
    "        model: CNN network\n",
    "        num_epochs: the number of training epochs\n",
    "        optimizer: optimize the loss function    loss_func.to(device)\n",
    "    loss_func.to(device)\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    # loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.to(device)\n",
    "    loss_func.to(device)\n",
    "    \n",
    "    # log train loss and test accuracy\n",
    "    losses = []\n",
    "    accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}/{}:'.format(epoch + 1, num_epochs))\n",
    "        # train step\n",
    "        loss = train(model, training_dataloader, loss_func, optimizer, device)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # evaluate step\n",
    "        accuracy = evaluate(model, test_dataloader, device)\n",
    "        accs.append(accuracy)\n",
    "        \n",
    "    \n",
    "    # show curve\n",
    "    show_curve(losses, \"train loss\")\n",
    "    show_curve(accs, \"test accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Documents/dataset_skin40/SAN\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(os.path.join(os.getcwd(),'SAN'))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelbest=torch.load('san19_pairwise/model/model_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> <class 'model.san.SAN'>\n"
     ]
    }
   ],
   "source": [
    "print(type(modelbest), type(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdict = modelbest['state_dict']\n",
    "for k,v in modelbest['state_dict'].items():\n",
    "    print(type(k),k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import san\n",
    "net = san(sa_type=0, layers=(3, 3, 4, 6, 3), kernels=[3, 7, 7, 7, 7], num_classes=1000)\n",
    "net.load_state_dict({k[7:]:v for k,v in modelbest['state_dict'].items()})\n",
    "# net = modelbest\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "net.fc = torch.nn.Sequential(torch.nn.Linear(2048, 4096),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(p=0.5),\n",
    "                                       torch.nn.Linear(4096, 4096),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(p=0.5),\n",
    "                                       torch.nn.Linear(4096, 40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in net.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Documents/dataset_skin40\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.split(os.getcwd())[0])\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 5\n",
    "lr =1e-6\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "fit(net, num_epochs, optimizer, device)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(dat, tar) in enumerate(test_dataloader):\n",
    "    print(i ,dat.shape, tar)\n",
    "    if ( i>30):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
