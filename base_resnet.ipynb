{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL,torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/SYSUDeepLearningSkin/Skin40\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "suffix = '%f' % time.time()\n",
    "log_dir = './log/%s'% suffix\n",
    "EXPERIMENT = 'resnet50_punish'\n",
    "\n",
    "datdir = os.path.join(os.getcwd(), 'Skin40')\n",
    "print(datdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinDataset(Dataset):\n",
    "    folds=5\n",
    "    def __init__(self, root , num_classes,fold=0,training=False,transform=None):\n",
    "        self.data_path = []\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(size=(224, 224)),\n",
    "                transforms.RandomRotation(5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean= [0.6075306,0.49116918 ,0.46066117],std = [0.22603881, 0.21623525, 0.2191065 ])\n",
    "            ]\n",
    "            )\n",
    "        for label in range(num_classes):\n",
    "            self.data_dir = os.path.join(root,os.listdir(root)[label])\n",
    "            self.filename = os.listdir(self.data_dir)\n",
    "            l = len(self.filename)\n",
    "            inter = l//SkinDataset.folds\n",
    "            picked = list(range(inter* fold,inter * (fold+1))) if not training else list(range(0,inter*fold))+list(range(inter*(fold+1),l))\n",
    "\n",
    "            for i in picked:\n",
    "                file_path = os.path.join(self.data_dir , self.filename[i])\n",
    "                self.data_path.append((file_path, label))\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        ddir , label = self.data_path[index]\n",
    "        img = Image.open(ddir)\n",
    "        result = (self.transform(img) , label)\n",
    "        del img\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SkinDataset(os.path.join(os.getcwd(),'Skin40'), 40,training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# del training_dataloader\n",
    "training_dataset = SkinDataset(os.path.join(os.getcwd(),'Skin40'), 40, training=True)\n",
    "test_dataset = SkinDataset(os.path.join(os.getcwd(),'Skin40'), 40)\n",
    "training_dataloader = DataLoader(training_dataset , batch_size = 128 , num_workers = 1, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset , batch_size = 128 , num_workers = 1, shuffle = True)\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_func, optimizer, device):\n",
    "    \"\"\"\n",
    "    train model using loss_fn and optimizer in an epoch.\n",
    "    model: CNN networks\n",
    "    train_loader: a Dataloader object with training data\n",
    "    loss_func: loss function\n",
    "    device: train on cpu or gpu device\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    # train the model using minibatch\n",
    "    print(len(train_loader))\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # forward\n",
    "#        print(i)\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, targets)\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print('train ok')\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    model: CNN networks\n",
    "    val_loader: a Dataloader object with validation data\n",
    "    device: evaluate on cpu or gpu device\n",
    "    return classification accuracy of the model on val dataset\n",
    "    \"\"\"\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "    # context-manager that disabled gradient computation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # record all target && correct prediction by each class\n",
    "        all_tar = np.zeros((40)).tolist()\n",
    "        cor_pre =  np.zeros((40)).tolist()\n",
    "\n",
    "        for i, (images, targets) in enumerate(val_loader):\n",
    "            # device: cpu or gpu\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # return the maximum value of each row of the input tensor in the \n",
    "            # given dimension dim, the second return vale is the index location\n",
    "            # of each maxium value found(argmax)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            cor_bol_list = (predicted == targets)\n",
    "            correct += (cor_bol_list).sum().item()\n",
    "            total += targets.size(0)\n",
    "            # \n",
    "            for i,bl in enumerate( cor_bol_list):\n",
    "                all_tar[targets[i].cpu().numpy().tolist()] += 1\n",
    "                if bl:\n",
    "                    cor_pre[predicted[i].cpu().numpy().tolist()] += 1\n",
    "        # accuracy of tatal && each class \n",
    "        accuracy = correct / total\n",
    "        each_acc = torch.tensor(cor_pre) / torch.tensor(all_tar)\n",
    "        return accuracy,each_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, num_epochs, optimizer, device):\n",
    "    \"\"\"\n",
    "     train and evaluate an classifier num_epochs times.\n",
    "    n and evaluate an classifier num_epochs times.\n",
    "    We use optimizer and cross entropy loss to train the model. \n",
    "    Args: \n",
    "        model: CNN network\n",
    "        num_epochs: the number of training epochs\n",
    "        optimizer: optimize the loss function    loss_func.to(device)\n",
    "    loss_func.to(device)\n",
    "\n",
    "    \"\"\"\n",
    "    # loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.to(device)\n",
    "    loss_func.to(device)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=log_dir + '/%s training loss' % EXPERIMENT)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)        \n",
    "        # tensorboard\n",
    "        # train step\n",
    "        loss = train(model, training_dataloader, loss_func, optimizer, device)\n",
    "        \n",
    "        # save model\n",
    "        torch.save(model.state_dict(), 'model_%s.pth' % EXPERIMENT+'%s' %suffix)\n",
    "        print('train evaluate')\n",
    "        accuracy,each_acc = evaluate(model, training_dataloader, device)   \n",
    "        print('train finish')\n",
    "        writer.add_scalar('training loss', loss, epoch)\n",
    "        writer.add_scalar('training acc', accuracy, epoch)        \n",
    "        for i in range(40):\n",
    "            temp_writer = SummaryWriter(log_dir=log_dir + '/%s' % EXPERIMENT + ' training class %s acc' % str(i))\n",
    "            temp_writer.add_scalar('training class acc',each_acc[i], epoch)\n",
    "            temp_writer.close()\n",
    "            \n",
    "        # evaluate step\n",
    "        accuracy,each_acc = evaluate(model, test_dataloader, device) \n",
    "        print('eval')\n",
    "        writer.add_scalar('testing acc', accuracy, epoch)\n",
    "        for i in range(40):\n",
    "            temp_writer = SummaryWriter(log_dir=log_dir + '/%s' % EXPERIMENT + 'testing class %s acc' % str(i))\n",
    "            writer.add_scalar('testing class acc',each_acc[i], epoch)\n",
    "            temp_writer.close()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n",
      "1\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n",
      "2\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n",
      "3\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n",
      "4\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n",
      "5\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n",
      "6\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n",
      "7\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n",
      "8\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n",
      "9\n",
      "15\n",
      "train ok\n",
      "train evaluate\n",
      "train finish\n",
      "eval\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "for parma in model.parameters():\n",
    "    parma.requires_grad = False\n",
    "model.fc = torch.nn.Sequential(nn.Linear(model.fc.in_features, 40))\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=lr,momentum=0.9)\n",
    "\n",
    "fit(model, num_epochs, optimizer, device)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([128, 3, 224, 224]) tensor([ 6, 18, 36, 31,  3, 15,  8,  3, 35,  7, 13, 36,  2, 28, 32, 37,  1, 21,\n",
      "         5, 33, 35, 16,  8,  9,  5, 25,  8,  2, 15, 17, 32, 31, 23, 28,  6, 20,\n",
      "        24, 15, 39, 36, 15, 24,  9, 16, 31, 16,  2, 17, 14, 29, 17, 35, 22, 25,\n",
      "        31, 10, 15, 11, 13, 16, 36, 29, 37, 15, 38, 24,  5, 29,  3,  3, 19,  7,\n",
      "        16, 29, 12, 16, 18, 18, 26, 10, 38, 26,  0,  8, 36, 20, 28,  9, 38, 10,\n",
      "        28, 35, 19, 27, 20, 33, 24, 31, 15, 33, 23, 14, 30,  1, 17,  9, 22, 13,\n",
      "        38, 29, 27, 37, 20, 38, 23, 13, 18,  2, 36, 23, 39, 21, 25,  6, 39, 34,\n",
      "         8, 19])\n",
      "1 torch.Size([128, 3, 224, 224]) tensor([24, 27, 24, 12,  9, 28, 11, 37, 22, 26, 38, 23,  0, 33, 38, 33, 18, 28,\n",
      "        32, 15,  3,  5, 21, 13, 15, 29, 28, 25, 28, 18,  3,  5, 22, 35, 14, 23,\n",
      "        32,  9, 34,  8,  2, 30, 16,  4, 22, 10,  7, 28, 26, 39, 22,  7, 28, 24,\n",
      "        23, 30, 24, 11, 21, 10, 30, 14, 34,  3, 33, 16, 34, 11, 37, 26, 39, 21,\n",
      "        19, 37,  6,  1, 13, 29, 19, 31,  1,  3, 27, 32, 34, 20, 34, 30, 32,  7,\n",
      "        12, 14,  9,  7,  5, 23,  2, 24, 35, 26,  4, 21, 39, 18,  8,  9, 36, 25,\n",
      "         2,  1, 13, 12,  4, 19, 32, 37, 10,  6, 22, 26, 27, 35,  6,  7,  3,  1,\n",
      "        12, 21])\n",
      "2 torch.Size([128, 3, 224, 224]) tensor([39, 25, 20, 22, 19, 12, 25,  8, 22, 33,  0, 18, 31, 12, 35, 27, 16, 23,\n",
      "        23, 11, 22, 27,  0, 34, 36, 12, 32, 11, 17, 32, 17, 14,  9,  4,  0,  6,\n",
      "         3, 39, 33, 27, 32, 22, 37,  9, 38, 15, 17, 11,  0,  3,  0,  5, 17, 31,\n",
      "        38, 39,  5, 37,  4, 16, 26, 15, 18, 11, 29, 31,  7,  4, 34, 13, 17, 14,\n",
      "        20, 29, 20, 21,  7, 36,  8, 22, 34, 24, 19, 33,  8,  8,  7, 20, 30, 12,\n",
      "        13, 36, 31, 27, 25,  6, 26, 31, 34, 14, 31, 33, 23, 14,  4, 17,  1, 24,\n",
      "        14, 18, 35, 28, 15, 32, 38, 23,  1,  8, 29,  2, 29, 30,  1,  5, 17,  1,\n",
      "         2, 29])\n",
      "3 torch.Size([96, 3, 224, 224]) tensor([30, 11,  6, 30, 21, 10, 37, 34, 19, 34,  5, 25,  0,  6, 35, 30, 12, 11,\n",
      "        35, 14, 19, 13, 26,  2,  5,  7,  0, 11,  7, 10, 33, 26, 14, 18, 27, 18,\n",
      "        25,  0,  4,  4, 10,  2, 36, 16, 38,  0, 21,  1,  9, 27,  4, 21, 32, 25,\n",
      "         4, 13, 27, 10, 24, 28, 10,  9,  0, 38, 35, 12,  1, 21, 37,  5, 12,  6,\n",
      "        11,  4, 30,  6,  3, 26, 33, 13, 19, 37, 17, 10, 39, 39, 30, 39, 36, 20,\n",
      "        19, 16,  2, 25, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "for i,(dat, tar) in enumerate(test_dataloader):\n",
    "    print(i ,dat.shape, tar)\n",
    "    if ( i>30):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
